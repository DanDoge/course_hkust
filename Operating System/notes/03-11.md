### scheduling

FCFS
- cannot determine time of process

RR
- run a time quantum
- not garantee fast finish

SJF
- or preemptive version SRTF
- the optimal
- may lead to starvation
- require to know CPU burst time

priority scheduling
- static assign priority
- SJF, FCFS: special case of priority scheduling
- multilevel queue
    - run high priority queue before low
    - in each queue: RR, PS, FCFS
    - resource assigned to each queue(e.g. CPU time)

MLFQ
- multilevel feedback queue
- address two drawback
    - know CPU burst time prior
    - starvation
- multiqueues
    - high -> low -> lower -> FCFS
    - first enter highest queue, if cannot finish in a fixed time quantum, moved to second highest queue
    - possible starvation if the highest queue keeps not empty
    - if new job arrives, preemption / non-preemption for lower level process
- not optimal in job finish time
- Windows, Solaris, BSD
- not require prior knowledge on CPU time
- handle short job fast
- each job can make progress
- but still possible starvation
    - reshuffling jobs to different queue periodically

thread scheduling
- LWP as API to user libraries, appears to be a virtual processor
- PCS: treat the LWP as processor, scheduling threads within a process to LWPs
- SCS: scheduling kernel threads to CPU core
- in one-to-one system, only SCS, for no need to schedule threads to LWP

multi-processor scheduling
- asymmetric multiprocessing: only one processor accesses the system data structures, other processor execute user code
    - sequentially access to kernel structures, no sync issues
- symmetric multiprocessing: each processor self-scheduling
    - possible race condition: ready queue shared by all processors
- processor affinity
    - process has affinity for processor on which it is running
    - specify the processor on which a process favor to run
    - soft affinity: can run a unfavored CPU core, causing performance cost
    - hard affinity: only run on a subset of processors
- NUMA
    - NUMA-aware OS: assign memory closes to the CPU on which a thread is running
- load balancing: keep workload evenly distributed
    - push migration: a special task periodically checks load on each processor, push task from overloaded CPU to idle / less-busy CPU
    - pull migration / load stealing: idle processor pulls waiting task from a busy processor
    - counteracts the benefits of processor affinity

hyper threading
- overlap / interleave computer cycle and memory stall cycle
- provides a illusion that we have two cores
- e.g. Amazon EC2, cores -> logical processor
- two levels of scheduling
    - OS dedicing which software thread to run a logical CPU
    - hardware decides which hard thread to run on a physical core

real-time CPU scheduling
- hard real-time systems: not missing the deadline -> EDF
- soft real-time systems: no guarantee when a task to be scheduled
- peroidic processes: time, deadline and period
- rate = 1 / period

rate montonic scheduling
- high rate job = higher priority

earliest deadline first scheduling
- eariler deadline = higher priority
- has preemption

algorithm evaluation
- deterministic modeling
    - make up a specific test case
    - run all algorithms
    - compare results
- queueing models
    - model e.g. job arrive time -> Poisson distribution
    - Little's formula: n = $\lambda$ * W
        - n: queue length, $\lambda$: arrival rate, W: wait time
- simulations
    - actual process execution -> trace tape -> different policies
