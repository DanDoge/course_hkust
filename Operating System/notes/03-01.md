### threads

motivations
- shared resourses within the threads
- reuse file descripters
- light-weight creation of threads
- implement multitasking

benefits
- responsiveness: continue execution while part of the proces is blocked
- resource sharing: easier than shared memory and message passing
- economy: cheaper than process creation, switching loew overhead
- scalability: take advantage of multicore architectures
    - multi CPU core in a chip / processor
    - multicore programming
        - challenges: divide computing tasks, balance, data spitting, data dependency, testing and debugging
        - parallism: more than one task simutaneously, data / task ~
            - data paralleism: same operation of different part of data
            - task paralleism: unique operation on different threads across cores
        - concurrency: more than one task making progress
            - multiplex CPU core over the time
        - Amdahl's law: speedup <= 1 / (S + (1 - S) / N), where S is the serial portion, N is the number of processing cores
            - take overhear into account, this speedup rate cannot be reached

thread
- single unique execution context
    - PC, registers, execution flags, stack
- Thread Control Block
    - exechution state(registers, PC, pointer to stack)
    - scheduling info
    - accounting info
    - various pointers
    - pointer to enclosing process
- Nachos: thread as a class including TCB
- OS keep track of TCBs, in protected memory region
- thread state
    - active component of the process
    - address space being the passive part
    - shared: global variables, heap, io
    - private: TCB, CPU registers, stack
- life cycle
    - new -> ready <-> running -> waiting (-> ready) ... running -> terminated
    - running -> ready: interrupt, passive suspension
    - each device has one waiting queue
    - different queue may have different scheduler policy

multithreaded programs
- embedded systems: single program, concurrent operations
- modern OS kernels: system thread pool
- database servers: access data to shared data by multiusers
- nerwork servers: concurrent requestf from different users
- parallel programming

user threads and kernel threads
- OS provide system libs to create threads
- OS provide a illusion that kernel thread is transparent
- many-to-one: kernel thread take turn to run user threads
    - no parallism
    - one thread block will block the kernel thread
- one-to-one: each user-level thread maps to kernel thread
    - highest degree of isolation
    - max degree of parallism
    - number of threads restricted
- many-to-many: pool of kernel thread running
    - what is the right ratio of user:kernel
    - create a suffcient number kernel thread
- two-level model
    - allows a user thread to be obund to kernel thread

threading issues
- semantics of fork and exec
    - fork: duplicate all threads or only the calling thread? two version of fork
    - exec: replace all threads
- signal handling
    - signal handler: OS default or user-defined handler
    - deliver to the caller thread / every thread / certain thread / a specific thread
    - cancellation: target thread
        - asynchronous / deferred cancellation
        - specify the mode  
            - OFF: not to be terminated, signal wait for the mode change
            - Deferred:
            - Asynchronous:
        - cancellation occurs on cancellation point, then invoke the cleanup handler

thread-local storage
- each thread have its own copy of data

scheduler activation
- lightweight process as virtual processor, which coresponce to a kernel thread
- scheduler activation provide upcalls, notify the kernel to prepare a kernel thread
- low level in kernel mode
